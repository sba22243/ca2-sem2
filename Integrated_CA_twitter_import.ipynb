{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10effb37-78e4-4382-83fc-6daa1599ba05",
   "metadata": {},
   "source": [
    "# MSc in Data Analytics\n",
    "# Sentiment Analysis\n",
    "\n",
    "Assessment Task\n",
    "Students are advised to review and adhere to the submission requirements documented after the assessment task.  \n",
    "In this continuous assessment, You are required to identify and carry out an analysis of a large dataset gleaned from the twitter API. Instructions for accessing the data can be found here  \n",
    "https://datascienceparichay.com/article/get-data-from-twitter-api-in-python-step-by-step-guide/  \n",
    "https://www.toptal.com/apache/apache-spark-streaming-twitter  \n",
    "OR You may use the data held here:  \n",
    "https://archive.org/details/twitterstream?sort=-publicdate  \n",
    "You must collect at least 1 year's tweets on a topic, this data should be stored as requested below, and you are then required to analyse any change sentiment that occurs over the time period that you have selected.  \n",
    "Following your analysis, you are then required to make a time series forecast of the sentiment at 1 week, 1 month and 3 months going forward. This forecast must be displayed as a dynamic dashboard.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37f51d2-b0f8-4081-82bb-f9b941ea2287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310fe401-7ba7-45ec-90cb-118ed285e3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import col\n",
    "import json, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43573d91-3b69-492f-b9e8-c2bd2127b8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# Topic\n",
    "topic = \"vaccine\"\n",
    "\n",
    "# Read the directory and find the bz2 files containint the tweets\n",
    "dataset_path = r'c:\\tmp\\twitter'\n",
    "\n",
    "\n",
    "# Size of the batch \n",
    "num_files = 100\n",
    "\n",
    "destination_folder = f'c:\\\\\\\\tmp\\\\twitter-result\\\\'\n",
    "#destination_folder = f'/twitter/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32a0dbf-a063-4bb7-89da-05542111bea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Spark Context\n",
    "# fix to manage the error \n",
    "# \"Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"sba22243-step1\").config(\"spark.sql.debug.maxToStringFields\", 100).getOrCreate()\n",
    "sqlContext = spark._wrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a29661c-d109-4509-b814-0f0efba63201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_tweets(filename):\n",
    "\n",
    "    file_list = [] # this list contain the filenames of the dataset stored in the dataset_path folder\n",
    "    # explore the subfolder\n",
    "    for subdir, dirs, files in os.walk(dataset_path):\n",
    "        for file in files:\n",
    "            # if the file is a bz2 then get the filename\n",
    "            if file.endswith('.bz2'):\n",
    "                filename_bz2 = os.path.join(subdir, file)\n",
    "                filename_bz2 = filename_bz2.replace('\\\\', '\\\\\\\\')\n",
    "                file_list.append(os.path.join(subdir, file))\n",
    "    print(f'Num of files {len(file_list)}')\n",
    "    #print(file_list)\n",
    "    \n",
    "    # Read a batch of files in parallel\n",
    "    for i in range(0, len(file_list), num_files):\n",
    "        sub_list = file_list[i:i+num_files]\n",
    "        dataframe = sqlContext.read.json(sub_list)\n",
    "\n",
    "        dataframe_result = dataframe.filter((col('lang') == 'en') & col('text').rlike(f'(?i){topic}'))\n",
    "        dataframe_result = dataframe_result.drop('entities', 'extended_entities', 'retweeted_status')\n",
    "\n",
    "        print(f'Baatch {i} - writing {dataframe_result.count()} tweets')\n",
    "        dataframe_result.write.mode(\"append\").json('file:///' + destination_folder + filename + '.json')\n",
    "        #dataframe_result.write.mode(\"append\").json('hdfs://hadoop.local/' + destination_folder + filename + '.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ff61b8-2858-4529-91e5-10b1112e1855",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_tweets('2021-january')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c001b02f-4721-4fcf-93de-d8f673542a62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
